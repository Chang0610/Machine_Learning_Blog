# Interview Questions and Answers

Below is a list of questions followed by detailed answers and explanations.

---

## Preliminary Questions

- **What is L1 and L2 regularization?**
- **What is cuda kernel? and deep learning system related project question**
- **why kernel size is odd number**
- **what's the difference between llm and neural network, pros and cons**
- **recommendation system project go through**
- **transformer related**
- **prenorm postnorm**
- **each layer like BatchNorm LayerNorm**
- **What is stable diffusition, and is different for maybe transfoemer?**
- **What is diffustion model?**
- **What is lora?**
- **why for attention score it will divide by sqrt d**

---

## C Language Compilation Process

The C compilation process typically involves four stages:

1. **Preprocessing**: Handles directives like `#include` and `#define`.
2. **Compilation**: Translates the code into assembly language.
3. **Assembly**: Converts the assembly code into machine code.
4. **Linking**: Combines object files into a single executable.

---

## Thread vs. Process

- **Process**: An independent program in execution with its own memory space.
- **Thread**: A subset of a process, sharing memory space with other threads within the same process.
- **Difference**: Threads have less overhead for context switching and share resources more readily, but processes are isolated, providing more robust memory protection.

### Pros and Cons of Processes

**Advantages:**
1. Processes are isolated from each other; one process crashing does not affect other processes.
2. Suitable for running completely independent tasks (e.g., several different programs).
3. Provides increased security; data sharing between processes requires explicit communication mechanisms.

**Disadvantages:**
1. Creating and switching processes incurs high overhead.
2. Interprocess communication (IPC) is complex and less efficient.

### Pros and Cons of Threads

**Advantages:**
1. Thread communication is simple and efficient because threads share the same memory space.
2. Lower overhead for creating and switching threads.
3. Better suited for tasks that require frequent communication and shared data (e.g., multithreaded servers).

**Disadvantages:**
1. Threads sharing memory can easily lead to data races (requiring synchronization mechanisms such as locks).
2. A crash in one thread can potentially cause the entire process to crash.

---

## GPU vs. CPU Differences

- **CPU**: Optimized for single-threaded performance with fewer cores.
- **GPU**: Contains thousands of smaller cores, optimized for parallel processing tasks (e.g., rendering graphics, deep learning).

---

## CUDA Out of Memory Solutions

1. **Reduce Batch Size**: Lower memory demand.
2. **Model Optimization**: Use smaller models or pruning techniques.
3. **Gradient Accumulation**: Accumulate gradients over multiple batches.

---

## MLP Forward and Backward Propagation

1. **Forward Propagation**:
   - Input is multiplied by weights, and bias is added.
   - Output is passed through an activation function.
2. **Backward Propagation**:
   - Error gradient is computed, weights are adjusted to minimize loss.

---

## TensorFlow Topology

TensorFlow is an open-source machine learning and deep learning framework. It is widely used for building and training machine learning modelsâ€”especially neural network models. TensorFlow constructs computation graphs to organize operations. It enables clear forward and backward computation paths, supporting both static and dynamic computation graphs.

### Static Computation Graph

Static computation graphs were the default in TensorFlow 1.x. In this mode, the computation graph must be defined before execution, and its structure remains fixed during runtime. This method is part of declarative programming.

**Characteristics:**
1. The graph definition and execution are separated:
   - First, the computation graph is defined; then, the graph is executed through a session (e.g., using `tf.Session`).
2. Defined once, run multiple times:
   - The graph structure is fixed and can be reused for multiple computations.
3. Efficiency:
   - The static graph undergoes optimizations (such as operation fusion and memory allocation optimizations) before running, leading to fast execution.

**Advantages:**
- High performance: The graph is optimized before execution, making it suitable for large-scale computations.
- Device distribution: You can explicitly specify which device (CPU/GPU/TPU) each operation runs on.
- Reusability: The same graph can be executed multiple times, reducing the overhead of repeated constructions.

**Disadvantages:**
- Increased development complexity: You must define the graph beforehand and then launch its execution, which can make debugging less intuitive.
- Lack of flexibility: Dynamic structures such as varying sequence lengths might not be easily supported.

### Dynamic Computation Graph

Dynamic computation graphs are the default mode in TensorFlow 2.x, often referred to as Eager Execution. In this mode, the computation graph is built on the fly as operations are executed, which is aligned with imperative programming.

**Characteristics:**
1. Immediate Execution:
   - Operations execute immediately as the code runs, without explicitly building a graph.
2. Flexibility:
   - Supports dynamic structures, such as variable-length sequences.
3. More intuitive debugging:
   - The code resembles standard Python code, making it easier to understand and debug.

**Advantages:**
- Ease of use: The code is more intuitive and avoids the complexity of defining and launching a computation graph.
- Flexibility: Suitable for dynamic tasks (e.g., loops and conditional logic).
- Debuggability: Allows step-by-step execution and inspection of intermediate results.

**Disadvantages:**
- Slightly lower performance: Since the graph is constructed on the fly, it lacks the pre-run optimizations of static graphs.

---

## Python Decorators

A decorator in Python is a function that wraps another function, adding functionality. Theyâ€™re useful for logging, timing, and access control, among other things.

---

## Logistic Regression

**Logistic Regression** is a statistical and machine learning method used for **binary classification** problems, where the goal is to predict one of two possible outcomes (e.g., Yes/No, 0/1, True/False). Despite its name, logistic regression is a **classification algorithm**, not a regression algorithm.

### Logistic (Sigmoid) Function

- Logistic regression uses the **sigmoid function** to map any input value to a probability between 0 and 1:  

  \[
  \sigma(z) = \frac{1}{1 + e^{-z}}
  \]

  where:
  - \( z = w^T x + b \) (the linear combination of features, weights, and bias).

---

## Cross-Validation

For each regularization value (e.g., \(\lambda = 0.1, 1, 10\)), the model is trained **5 times** during cross-validation when using 5-fold cross-validation.

- **Dataset Splitting:** Divide the dataset into 5 subsets.
- **Training and Validation Loop:**
  - Each time, select 1 subset as the validation set and use the remaining 4 subsets as the training set.
  - Repeat this process 5 times, each time with a different subset as the validation set.
- **Averaging Performance:** Calculate the average performance (such as accuracy or error) over the 5 validations as a performance metric.

**Purpose:** Training the model 5 times ensures the evaluation takes into account how well the regularization value generalizes across different portions of the data.

---

## Evaluation Metrics for Classification vs. Regression

- **Classification:**
  - Use **F1 Score** for imbalanced datasets.
  
    \[
    F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \]
  
  - **Precision:** \(\text{True Positive (TP)}/(\text{False Positive (FP)} + \text{True Positive (TP)})\)
  - **Recall:** \(\text{True Positive (TP)}/(\text{False Negative (FN)} + \text{True Positive (TP)})\)

- **Regression:**
  - Use **MAE** (Mean Absolute Error), **MSE** (Mean Squared Error), or **RMSE** (Root Mean Squared Error) depending on whether you prioritize interpretability or sensitivity to large errors.

---

## What is Regularization in ML??

Regularization is a technique used to prevent overfitting in ML models. Overfitting occurs when a model is too complex and fits the training data too closely, leading to poor performance on unseen data. Regularization techniques add a penalty term to the model's objective function, which encourages the model to find simpler solutions by reducing the magnitude of the model's coefficients or parameters. This reduction in complexity helps prevent the model from fitting noise in the training data and improves generalization. Popular regularization techniques include L1 (lasso) and L2 (ridge) regularization.

---

## Interview Question 1

**What is cross validation?**

Cross validation is a method used to evaluate a modelâ€™s performance and avoid overfitting. It is an easy method to compare the predictive capabilities of models and is especially useful when only limited data is available.

---

## Interview Question 1

**What is P-value?**

P-value, or probability value, indicates the likelihood of obtaining the observed dataâ€”or something more extremeâ€”by random chance. A small P-value suggests that the observed result is unlikely to have occurred by chance, providing evidence to support the alternative hypothesis over the null hypothesis.

---

## Interview Question 1

**What are the types of Machine Learning?**

There are mainly three types of Machine Learning:

1. **Reinforcement Learning:**  
   Focuses on taking the best possible action to maximize a reward in a given scenario. It is used by various systems and machines to determine the optimal path or strategy.  
   
   > **Examples:**
   > - **Training a Robot to Navigate a Maze:**  
   >    - **Agent:** The robot.  
   >    - **Environment:** The maze.  
   >    - **Actions:** Move up, down, left, or right.  
   >    - **Reward:** Positive for reaching the goal; negative for hitting walls.
   > - **Playing Video Games:** Agents learn strategies to win games (e.g., AlphaGo).

2. **Supervised Learning:**  
   Uses labeled datasets to train algorithms to classify data or predict outcomes accurately.  
   
   > **Examples:**
   > - **Classification:** Predicting whether an email is spam or not.  
   >   - **Input:** Email text and features (e.g., keywords, sender).  
   >   - **Output:** Label ("Spam" or "Not Spam").
   > - **Regression:** Predicting house prices based on features like size, location, and age.  
   >   - **Input:** Features (size, location, age).  
   >   - **Output:** Continuous value (price).

3. **Unsupervised Learning:**  
   Deals with unlabeled data to discover hidden patterns or structures within the data.
   
   > **Examples:**
   > - **Clustering:** Grouping customers with similar purchase behaviors (customer segmentation).  
   >   - **Input:** Customer features (purchase history, age, location).  
   >   - **Output:** Clusters (e.g., high-spending, low-spending groups).
   > - **Dimensionality Reduction:** Reducing the number of features while preserving meaningful information (e.g., PCA, t-SNE).

---

## Interview Question 3

**What are five popular algorithms used in Machine Learning?**

1. **Neural Networks:**  
   A set of algorithms designed to help machines recognize patterns without explicit programming.

2. **Decision Trees:**  
   A supervised learning technique where internal nodes represent features, branches represent decision rules, and leaf nodes represent outcomes.

3. **K-Nearest Neighbors (KNN):**  
   A supervised learning algorithm for classification and regression that predicts the target for new data points based on the k-closest training examples.

4. **Support Vector Machines (SVM):**  
   An algorithm that finds the best hyperplane to separate n-dimensional space into classes for quick classification.

5. **Probabilistic Networks (Graphical Models):**  
   Models that use graphs (such as Bayesian networks and Markov random fields) to represent conditional dependencies between variables.

> **Probabilistic Network Instance â€“ Medical Diagnosis:**  
> **Scenario Description:**  
> A doctor uses a patient's symptoms to diagnose whether they have a cold or pneumonia while also considering recent exposure to a virus.  
>  
> **Random Variables:**  
> - **A:** Whether the patient has been exposed to a virus (Yes/No).  
> - **B:** Whether the patient has a cold (Yes/No).  
> - **C:** Whether the patient has pneumonia (Yes/No).  
> - **D:** Whether the patient has a fever (Yes/No).  
> - **E:** Whether the patient has a cough (Yes/No).  
>  
> **Causal Relationships:**  
> - Exposure (A) affects the probabilities of catching a cold (B) and pneumonia (C).  
> - Having a cold (B) or pneumonia (C) leads to fever (D) and cough (E).  
>  
> **Bayesian Network Structure:**  
> The relationships among these variables can be represented as a directed acyclic graph (DAG):  
>  
> ```
>     A
>    / \
>   B   C
>  / \ / \
> D   E   D
> ```  
>  
> **Joint Probability Distribution:**  
> The Bayesian network allows the joint probability \( P(A,B,C,D,E) \) to be factorized as:  
>  
> \[
> P(A,B,C,D,E) = P(A) \cdot P(B \mid A) \cdot P(C \mid A) \cdot P(D \mid B,C) \cdot P(E \mid B,C)
> \]  
>  
> **Inference Process:**  
> For example:  
> - \( P(A = \text{Yes}) = 0.3 \) (30% probability of virus exposure).  
> - \( P(B = \text{Yes} \mid A = \text{Yes}) = 0.8 \) and \( P(B = \text{Yes} \mid A = \text{No}) = 0.2 \).  
> - \( P(C = \text{Yes} \mid A = \text{Yes}) = 0.6 \) and \( P(C = \text{Yes} \mid A = \text{No}) = 0.1 \).  
> - \( P(D = \text{Yes} \mid B = \text{Yes}, C = \text{Yes}) = 0.95 \), etc.  
>  
> Using these conditional probabilities, one can compute:  
> - The probability of the patient having a fever \( P(D = \text{Yes}) \).  
> - Given that the patient has a fever, the probability of having pneumonia \( P(C = \text{Yes} \mid D = \text{Yes}) \).  
>  
> **Application:**  
> Bayesian networks can help doctors calculate the likelihood of a cold or pneumonia based on symptoms such as fever and cough, thus aiding in diagnosis.

---

## Interview Question 4

**What is a neural network?**

A neural network is structured similarly to the human brain; it consists of interconnected neurons that help information flow from one neuron to another. It represents a function that maps input data to a desired output using a set of weights and biases. Structurally, it is organized into an input layer, one or more hidden layers, and an output layer.

---

## Interview Question 1

**You have come across some missing data in your dataset. How will you handle it?**

**Answer:**

To handle missing or corrupted data, a common approach is to replace the corresponding rows or columns containing the faulty data with some alternative values. Two very useful functions in Pandas for this purpose are:
- `isnull()`: Used to identify missing values in the dataset.
- `fillna()`: Used to fill missing values (for example, filling with 0â€™s).

---

## Interview Question 1

**What different targets do classification and regression algorithms require?**

- **Regression algorithms** require numerical targets. Regression finds correlations between independent and dependent variables, helping predict continuous variables such as market trends or weather patterns.
- In **classification**, algorithms categorize data into classes based on various parameters. For example, predicting whether bank customers will pay their loans, classifying emails as spam, or diagnosing medical conditions.

---

## Interview Question 1

**What is the difference between Regression and Classification?**

- **Classification** produces discrete outcomes, categorizing data into separate classes.
- **Regression** evaluates the relationship between independent variables and a continuous dependent variable.

---

## Interview Question 2

**Explain Decision Tree Classification**

A decision tree uses a tree-like model of decisions and their possible consequences for regression or classification tasks. The dataset is repeatedly split into smaller subsets by following a tree-like structure with branches representing decision rules and nodes representing outcomes. Decision trees can handle both categorical and numerical data.

---

## Interview Question 3

**What is the confusion matrix?**

A confusion matrix is an error matrix used to evaluate the performance of a classification algorithm. It determines the classifierâ€™s performance on a given test dataset by showing the counts of correct and incorrect predictions.

### Structure of the Confusion Matrix

Using a binary classification task as an example, a confusion matrix is typically a 2Ã—2 table where the rows represent the actual classes and the columns represent the predicted classes:

| Actual \ Predicted      | Predicted Positive | Predicted Negative |
|-------------------------|--------------------|--------------------|
| Actual Positive         | True Positive (TP) | False Negative (FN)|
| Actual Negative         | False Positive (FP)| True Negative (TN) |

---

## Interview Question 3

**How is a logistic regression model evaluated?**

One effective way to evaluate a logistic regression model is using a confusion matrix. This matrix allows the calculation of evaluation metrics such as Accuracy, Precision, Recall, and the F1 Score.  

- **Low Recall:** Indicates too many False Negatives.
- **Low Precision:** Indicates too many False Positives.

To balance precision and recall effectively, the F1 Score is used.

---

## ðŸŒŸ Interview Question 4

**To start Linear Regression, you would need to make some assumptions. What are those assumptions?**

To initiate a Linear Regression model, you need to assume:
- The model's residuals have a multivariate normal distribution.
- There is no autocorrelation.
- Homoscedasticity: The variance of the dependent variable is consistent across all data points.
- A linear relationship exists between independent and dependent variables.
- There is little to no multicollinearity among the independent variables.

---

## ðŸŒŸ Interview Question 4

**How would you define collinearity?**

Collinearity occurs when two predictor variables in a multiple regression exhibit some correlation with each other.

---

## ðŸŒŸ Interview Question 5

**What is multicollinearity and how will you handle it in your regression model?**

Multicollinearity exists when the independent variables in a regression model are correlated, despite the assumption that they should be independent. This high correlation can cause issues when fitting the model.

One common method to check for multicollinearity is by calculating the Variance Inflation Factor (VIF).  
- If VIF is less than 4, multicollinearity is usually not a concern.
- If VIF exceeds 4, further investigation is needed.
- If VIF exceeds 10, there are serious concerns and model adjustments may be required.

---

## ðŸŒŸ Interview Question 6

**What are support vectors in SVM (Support Vector Machine)?**

Support vectors are the data points that lie closest to the hyperplane (the decision boundary) in an SVM. These points are critical because they define the position and orientation of the hyperplane used for classification.

> **XGBoost Simplified Workflow (Regression Example):**  
> 1. **Initialization:**  
>    - Start with a simple prediction (e.g., the average of all values).  
> 2. **Compute Residuals:**  
>    - Calculate the residual for each data point (the difference between the actual and the predicted value).  
> 3. **Train a New Tree:**  
>    - Build a decision tree to fit these residuals.  
> 4. **Update Prediction:**  
>    - Add the new tree's predictions to the original predictions to obtain updated values.  
> 5. **Iterate:**  
>    - Repeat the process until reaching the desired number of trees or convergence.

---

## ðŸŒŸ Interview Question 6

**Explain why the performance of XGBoost is better than that of SVM?**

XGBoost is an ensemble approach that utilizes multiple decision trees. Its iterative boosting mechanism helps it improve performance by focusing on correcting mistakes made by previous trees. In contrast, if data are not linearly separable, an SVM (a linear separator) must use a kernel to map inputs into a higher-dimensional space. Since no single kernel works optimally for every dataset, SVM performance can be limited.

---

## Interview Question 7

**Why is an encoder-decoder model used for NLP?**

An encoder-decoder model is used in NLP to generate an output sequence based on an input sequence. The encoder processes the input and produces a final state that is passed to the decoder, which then generates the output sequence. This structure is particularly powerful for tasks like machine translation, as it allows dynamic handling of sequences of varying lengths.

---

## Interview Question 10

**What is Selection Bias?**

Selection Bias is a statistical error that occurs when the sampling method favors one subset of the population over others, leading to inaccurate conclusions.

---

## Interview Question 11

**What is the difference between correlation and causality?**

- **Correlation** is a measure of the relationship between two variables, where one does not necessarily cause the other.
- **Causality** indicates that one variable directly affects or causes the change in another variable.

---

## Interview Question 12

**What is the difference between Correlation and Covariance?**

- **Correlation:** Quantifies the strength and direction of the relationship between two variables, typically ranging between -1 and 1.
- **Covariance:** Measures how two variables vary together (i.e., how changes in one variable are associated with changes in another).

---

## Interview Question 13

**What are the differences between Type I error and Type II error?**

| Type I Error                               | Type II Error                             |
|--------------------------------------------|-------------------------------------------|
| False positive                             | False negative                            |
| An error where something is reported as having occurred when it did not. | An error where it is reported that nothing has occurred when it actually has. |

---

## Interview Question 14

**What is semi-supervised learning?**

Semi-supervised learning is an approach in which a small amount of labeled data is used to guide the learning process on a much larger amount of unlabeled data. This method combines the efficiency of unsupervised learning with the effectiveness of supervised learning.

---

## Interview Question 15

**Where is semi-supervised learning applied?**

It is applied in areas such as data labeling, fraud detection, and machine translation.

---

## Interview Question 15

**What is ensemble learning?**

Ensemble learning is a method that combines multiple machine learning models to produce a more powerful and accurate overall model. The idea is that a group of modelsâ€”when combinedâ€”provides better performance than any single model alone.

---

## Interview Question 15

**What is the difference between supervised and reinforcement learning?**

- **Supervised Learning:** Algorithms are trained using labeled data and predict a specified output.
- **Reinforcement Learning:** Algorithms are trained using a reward function and learn by taking actions to maximize cumulative rewards.

---

## Interview Question 15

**What are the requirements of reinforcement learning environments?**

A reinforcement learning environment consists of:
- **State:** The current condition or situation in the environment.
- **Reward:** The feedback received after taking an action.
- **Agent:** The algorithm or entity that takes actions.
- **Environment:** The simulation or task with which the agent interacts.

---

### Key Concepts in Reinforcement Learning

1. **Agent:**  
   The learning entity that performs actions, such as a robot or a game player.
2. **Environment:**  
   The external system the agent interacts with to obtain states and rewards.
3. **State (S):**  
   The current status of the environment, which the agent uses to decide its next action.
4. **Action (A):**  
   The behavior performed by the agent in the current state.
5. **Reward (R):**  
   The feedback from the environment to evaluate the agent's action, which can be positive (reward) or negative (punishment).
6. **Policy (Ï€):**  
   A rule or function that determines the action to take in each state.
7. **Value Function (V(s)):**  
   The expected cumulative reward for the agent from a given state.
8. **Action-Value Function (Q(s, a)):**  
   The expected cumulative reward after taking action \(a\) in state \(s\).

---

## ðŸŒŸ Interview Question 16

**What is the Bayesian Network?**

A Bayesian network is a graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). It is probabilistic in nature, relying on probability distributions for prediction, reasoning, diagnostic tasks, and anomaly detection.

---

## ðŸŒŸ Interview Question 17

**What is another name for a Bayesian Network?**

Other common names include Casual Network, Belief Network, Bayes Network, Bayes Net, and Belief Propagation Network.

---

## Interview Question 18

**What is sensitivity?**

Sensitivity is the probability that the model correctly predicts a positive outcome when the actual value is positive. It serves as a metric for evaluating the model's ability to detect true positives.

\[
\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]

---

## Interview Question 19

**What is specificity?**

Specificity is the probability that the model correctly predicts a negative outcome when the actual value is negative. It measures the modelâ€™s ability to detect true negatives.

\[
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}
\]

---

## ðŸŒŸ Interview Question 20

**What techniques are used to find resemblance in the recommendation system?**

Techniques such as Cosine Similarity and Pearson Correlation are used to measure resemblance in recommendation systems.  
- **Pearson Correlation:** Computes the covariance between two vectors divided by the product of their standard deviations.  
- **Cosine Similarity:** Measures the cosine of the angle between two vectors.

---

## ðŸŒŸ Interview Question 21

**What does the area under the ROC curve indicate?**

The Area Under the ROC Curve (AUC) measures the testâ€™s ability to distinguish between classes.  
- A higher AUC indicates that the model is better at distinguishing between positive and negative classes.
- A value of 0.5 suggests the model performs no better than random guessing, while a value of 1.0 indicates perfect classification.

---

## ðŸŒŸ Interview Question 22

**What is clustering?**

Clustering is the process of grouping a set of objects into clusters so that objects within the same cluster are more similar to each other than to those in other clusters. It is widely used for tasks such as customer segmentation, image and text classification, anomaly detection, and building recommendation systems.

---

## ðŸŒŸ Interview Question 23

**List the differences between KNN and k-means clustering.**

| KNN                                    | K-means Clustering                      |
|----------------------------------------|-----------------------------------------|
| Used for classification and regression | Used for clustering                     |
| Supervised learning technique          | Unsupervised learning technique         |

Although both KNN (K-Nearest Neighbors) and K-Means clustering involve the parameter *k*, they are fundamentally different in terms of purpose, functioning, and application.

| **Difference Dimension**                | **KNN (K-Nearest Neighbors)**                                           | **K-Means Clustering**                                  |
|-----------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------|
| **Algorithm Type**                      | Supervised learning algorithm (used for classification or regression).  | Unsupervised learning algorithm (used for clustering tasks). |
| **Input Data**                          | Requires labeled data (input features and corresponding labels).         | Works on unlabeled data.                                |
| **Objective**                           | Predicts the class or value of a new data point based on the labels or values of the k-nearest points. | Groups data points into k clusters based on similarity. |
| **Parameter k**                         | Indicates the number of nearest neighbors considered for prediction.     | Indicates the number of clusters to form.             |
| **Distance Metric**                     | Used to find the k-nearest neighbors for classification or regression.   | Used to calculate the distance between data points and the cluster centers. |
| **Training Phase**                      | No explicit training phase; it uses the training data directly for predictions (lazy learning). | Requires an iterative training phase where the cluster centers are updated. |
| **Applications**                        | - Classification (e.g., spam detection).<br>- Regression (e.g., predicting housing prices). | - Clustering (e.g., customer segmentation based on purchasing behavior).<br>- Document clustering based on similarity. |
| **Output**                              | Provides a definitive predicted class or value.                          | Divides the dataset into k clusters (groups).         |
| **Scalability**                         | Can be computationally expensive for large datasets since distances to all training points are computed. | Typically more efficient as clusters centers are iteratively updated. |
| **Requires Supervision**                | Yes, because it relies on labeled data.                                  | No, as it is used on unlabeled data.                    |

---

## ðŸŒŸ Interview Question 24

**What is the time series?**

A time series is a sequence of data points collected or recorded at successive points in time, typically at equally spaced intervals. It is used for predicting future values based on the historical pattern of the target variable. Time series analysis is common in fields such as signal processing, engineering (communications and control systems), and weather forecasting.

---

## Interview Question 27

**What is an Outlier?**

An outlier is an observation that significantly deviates from the other observations in a dataset. While often considered errors, outliers can also provide insight into unique occurrences or rare events in the data.

---

## Interview Question 27

**What is dimension reduction in ML?**

Dimension reduction refers to the process of reducing the number of input variables (features) under consideration while retaining as much informational content as possible. This is done to improve the performance of learning algorithms, simplify models, or facilitate data visualization.

---

## ðŸŒŸ Interview Question 27

**What is a PCA?**

PCA (Principal Component Analysis) is a statistical technique used to reduce the dimensionality of large datasets while preserving as much variance as possible. It identifies patterns and correlations among features by transforming the original variables into a smaller number of uncorrelated variables called principal components. PCA is widely used for data preprocessing, exploratory data analysis, outlier detection, and noise reduction.

---

## ðŸŒŸ Interview Question 27

**What are the differences between stochastic gradient descent (SGD) and gradient descent (GD)?**

- **Gradient Descent (GD):**
  - Uses the entire dataset to compute the gradient, leading to more stable and precise updates, but it can be computationally inefficient for large datasets.
  - Suited for small datasets or scenarios where stability is paramount.

- **Stochastic Gradient Descent (SGD):**
  - Uses a single data point (or a small batch) at each iteration to compute the gradient, which speeds up the updates but introduces more noise.
  - Suitable for large datasets or online learning scenarios.

- **Mini-Batch SGD:**  
  Combines the benefits of GD and SGD:
  - At each update, a small batch (e.g., 32 data points) is used to compute the gradient.
  - The number of updates per epoch is approximately (total number of data points / batch size).
  - This approach balances computational efficiency and convergence stability and is widely used in practice.

---

## ðŸŒŸ Interview Question 25

**What is stemming?**

Stemming is a text normalization technique that removes affixes from words, reducing them to their base or root form. This process makes the text easier to process and is commonly used in information retrieval, text pre-processing, and text mining applications.

---

## ðŸŒŸ Interview Question 26

**What is Lemmatization?**

Lemmatization is a text normalization technique that converts words into their base, dictionary form (lemma). Unlike stemming, lemmatization considers the context and produces a valid word as the output. Although more computationally intensive than stemming, lemmatization usually results in more meaningful text representations in NLP tasks.

---

## Interview Question 27

**What is an Array?**

An array is a collection of elements of the same type (such as integers, strings, or floating-point numbers) stored in contiguous memory locations. Each element in the array is accessible by its index.

---

## Interview Question 27

**What is a linked list?**

A linked list is an ordered collection of elements of the same data type, where each element (node) contains the data and a pointer to the next node in the sequence. Unlike arrays, linked lists do not store elements in contiguous memory, which allows for efficient insertions and deletions.
